{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dq-tr8Vw4sep",
        "outputId": "f3c9c00e-afb6-4409-84e0-151146d68cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.1 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3e7a2321-ab29-4899-ab8e-48bdc012cd5f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3e7a2321-ab29-4899-ab8e-48bdc012cd5f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving SPBERT2.zip to SPBERT2.zip\n",
            "Archive:  SPBERT2.zip\n",
            "  inflating: spbert-main/.gitignore  \n",
            "   creating: spbert-main/__pycache__/\n",
            "  inflating: spbert-main/__pycache__/model.cpython-310.pyc  \n",
            "  inflating: spbert-main/BERT2SPBERT.drawio  \n",
            "   creating: spbert-main/EventQA/\n",
            "   creating: spbert-main/EventQA/de/\n",
            "  inflating: spbert-main/EventQA/de/data.lang  \n",
            "  inflating: spbert-main/EventQA/de/data.sparql  \n",
            "   creating: spbert-main/EventQA/en/\n",
            "  inflating: spbert-main/EventQA/en/data.lang  \n",
            "  inflating: spbert-main/EventQA/en/data.sparql  \n",
            "   creating: spbert-main/EventQA/pt/\n",
            "  inflating: spbert-main/EventQA/pt/data.lang  \n",
            "  inflating: spbert-main/EventQA/pt/data.sparql  \n",
            "   creating: spbert-main/fairseq_script/\n",
            "   creating: spbert-main/fairseq_script/nl2sparql/\n",
            "  inflating: spbert-main/fairseq_script/nl2sparql/__init__.py  \n",
            "  inflating: spbert-main/fairseq_script/nl2sparql/bert_tokenize.py  \n",
            "  inflating: spbert-main/fairseq_script/nl2sparql/create_bert_vocabulary.py  \n",
            "  inflating: spbert-main/fairseq_script/nl2sparql/dictionary_with_bert.py  \n",
            "  inflating: spbert-main/fairseq_script/nl2sparql/preprocess.py  \n",
            "  inflating: spbert-main/fairseq_script/nl2sparql/transformer_with_pretrained_bert.py  \n",
            "  inflating: spbert-main/fairseq_script/nl2sparql/translation_with_bert.py  \n",
            "  inflating: spbert-main/fairseq_script/TEST.bash  \n",
            "  inflating: spbert-main/fairseq_script/TRAIN0.sh  \n",
            "  inflating: spbert-main/fairseq_script/TRAIN0_withBERTvocab.sh  \n",
            "  inflating: spbert-main/fairseq_script/TRAIN1.sh  \n",
            "  inflating: spbert-main/fairseq_script/TRAIN2.bash  \n",
            "  inflating: spbert-main/fairseq_script/TRAIN3.bash  \n",
            "  inflating: spbert-main/generator_utils.py  \n",
            "   creating: spbert-main/LCQUAD/\n",
            "  inflating: spbert-main/LCQUAD/dev.en  \n",
            "  inflating: spbert-main/LCQUAD/dev.en-sparql.json  \n",
            "  inflating: spbert-main/LCQUAD/dev.sparql  \n",
            "  inflating: spbert-main/LCQUAD/test.en  \n",
            "  inflating: spbert-main/LCQUAD/test.en-sparql.json  \n",
            "  inflating: spbert-main/LCQUAD/test.sparql  \n",
            "  inflating: spbert-main/LCQUAD/train.en  \n",
            "  inflating: spbert-main/LCQUAD/train.en-sparql.json  \n",
            "  inflating: spbert-main/LCQUAD/train.sparql  \n",
            "  inflating: spbert-main/model.py    \n",
            "   creating: spbert-main/Mon/\n",
            "  inflating: spbert-main/Mon/dev.en  \n",
            "  inflating: spbert-main/Mon/dev.en-sparql.json  \n",
            "  inflating: spbert-main/Mon/dev.sparql  \n",
            "  inflating: spbert-main/Mon/test.en  \n",
            "  inflating: spbert-main/Mon/test.en-sparql.json  \n",
            "  inflating: spbert-main/Mon/test.sparql  \n",
            "  inflating: spbert-main/Mon/train.en  \n",
            "  inflating: spbert-main/Mon/train.en-sparql.json  \n",
            "  inflating: spbert-main/Mon/train.sparql  \n",
            "   creating: spbert-main/Mon50/\n",
            "  inflating: spbert-main/Mon50/dev.en  \n",
            "  inflating: spbert-main/Mon50/dev.en-sparql.json  \n",
            "  inflating: spbert-main/Mon50/dev.sparql  \n",
            "  inflating: spbert-main/Mon50/test.en  \n",
            "  inflating: spbert-main/Mon50/test.en-sparql.json  \n",
            "  inflating: spbert-main/Mon50/test.sparql  \n",
            "  inflating: spbert-main/Mon50/train.en  \n",
            "  inflating: spbert-main/Mon50/train.en-sparql.json  \n",
            "  inflating: spbert-main/Mon50/train.sparql  \n",
            "   creating: spbert-main/Mon80/\n",
            "  inflating: spbert-main/Mon80/dev.en  \n",
            "  inflating: spbert-main/Mon80/dev.en-sparql.json  \n",
            "  inflating: spbert-main/Mon80/dev.sparql  \n",
            "  inflating: spbert-main/Mon80/test.en  \n",
            "  inflating: spbert-main/Mon80/test.en-sparql.json  \n",
            "  inflating: spbert-main/Mon80/test.sparql  \n",
            "  inflating: spbert-main/Mon80/train.en  \n",
            "  inflating: spbert-main/Mon80/train.en-sparql.json  \n",
            "  inflating: spbert-main/Mon80/train.sparql  \n",
            "  inflating: spbert-main/moses.py    \n",
            " extracting: spbert-main/myFile.txt  \n",
            "  inflating: spbert-main/NL2SPARQL Workflow  \n",
            "   creating: spbert-main/notebooks/\n",
            "  inflating: spbert-main/notebooks/NormalizeKeywordSPARQL.ipynb  \n",
            "  inflating: spbert-main/preprocessing.py  \n",
            "  inflating: spbert-main/preprocessing_multilang.py  \n",
            "  inflating: spbert-main/PretrainingObjectives.drawio  \n",
            "   creating: spbert-main/QALD/\n",
            "   creating: spbert-main/QALD/de/\n",
            "  inflating: spbert-main/QALD/de/test.lang  \n",
            "  inflating: spbert-main/QALD/de/test.sparql  \n",
            "  inflating: spbert-main/QALD/de/train.lang  \n",
            "  inflating: spbert-main/QALD/de/train.sparql  \n",
            "   creating: spbert-main/QALD/en/\n",
            "  inflating: spbert-main/QALD/en/test.lang  \n",
            "  inflating: spbert-main/QALD/en/test.sparql  \n",
            "  inflating: spbert-main/QALD/en/train.lang  \n",
            "  inflating: spbert-main/QALD/en/train.sparql  \n",
            "   creating: spbert-main/QALD/es/\n",
            "  inflating: spbert-main/QALD/es/test.lang  \n",
            "  inflating: spbert-main/QALD/es/test.sparql  \n",
            "  inflating: spbert-main/QALD/es/train.lang  \n",
            "  inflating: spbert-main/QALD/es/train.sparql  \n",
            "   creating: spbert-main/QALD/fa/\n",
            "  inflating: spbert-main/QALD/fa/test.lang  \n",
            "  inflating: spbert-main/QALD/fa/test.sparql  \n",
            "  inflating: spbert-main/QALD/fa/train.lang  \n",
            "  inflating: spbert-main/QALD/fa/train.sparql  \n",
            "   creating: spbert-main/QALD/fr/\n",
            "  inflating: spbert-main/QALD/fr/test.lang  \n",
            "  inflating: spbert-main/QALD/fr/test.sparql  \n",
            "  inflating: spbert-main/QALD/fr/train.lang  \n",
            "  inflating: spbert-main/QALD/fr/train.sparql  \n",
            "   creating: spbert-main/QALD/hi_IN/\n",
            "  inflating: spbert-main/QALD/hi_IN/test.lang  \n",
            "  inflating: spbert-main/QALD/hi_IN/test.sparql  \n",
            "  inflating: spbert-main/QALD/hi_IN/train.lang  \n",
            "  inflating: spbert-main/QALD/hi_IN/train.sparql  \n",
            "   creating: spbert-main/QALD/it/\n",
            "  inflating: spbert-main/QALD/it/test.lang  \n",
            "  inflating: spbert-main/QALD/it/test.sparql  \n",
            "  inflating: spbert-main/QALD/it/train.lang  \n",
            "  inflating: spbert-main/QALD/it/train.sparql  \n",
            "   creating: spbert-main/QALD/nl/\n",
            "  inflating: spbert-main/QALD/nl/test.lang  \n",
            "  inflating: spbert-main/QALD/nl/test.sparql  \n",
            "  inflating: spbert-main/QALD/nl/train.sparql  \n",
            "   creating: spbert-main/QALD/pt/\n",
            "  inflating: spbert-main/QALD/pt/test.lang  \n",
            "  inflating: spbert-main/QALD/pt/test.sparql  \n",
            "  inflating: spbert-main/QALD/pt/train.lang  \n",
            "  inflating: spbert-main/QALD/pt/train.sparql  \n",
            "   creating: spbert-main/QALD/pt_BR/\n",
            "  inflating: spbert-main/QALD/pt_BR/test.lang  \n",
            "  inflating: spbert-main/QALD/pt_BR/test.sparql  \n",
            "  inflating: spbert-main/QALD/pt_BR/train.lang  \n",
            "  inflating: spbert-main/QALD/pt_BR/train.sparql  \n",
            "   creating: spbert-main/QALD/ro/\n",
            "  inflating: spbert-main/QALD/ro/test.lang  \n",
            "  inflating: spbert-main/QALD/ro/test.sparql  \n",
            "  inflating: spbert-main/QALD/ro/train.lang  \n",
            "  inflating: spbert-main/QALD/ro/train.sparql  \n",
            "   creating: spbert-main/QALD/ru/\n",
            "  inflating: spbert-main/QALD/ru/test.lang  \n",
            "  inflating: spbert-main/QALD/ru/test.sparql  \n",
            "  inflating: spbert-main/QALD/ru/train.lang  \n",
            "  inflating: spbert-main/QALD/ru/train.sparql  \n",
            "  inflating: spbert-main/README.md   \n",
            "  inflating: spbert-main/requirements.txt  \n",
            "  inflating: spbert-main/run.py      \n",
            "  inflating: spbert-main/seperate_en_sparql.py  \n",
            "  inflating: spbert-main/seperate_multilang_sparql.py  \n",
            "   creating: spbert-main/VQUANDA/\n",
            "   creating: spbert-main/VQUANDA/cover_processed/\n",
            "  inflating: spbert-main/VQUANDA/cover_processed/dev.sparql  \n",
            "  inflating: spbert-main/VQUANDA/cover_processed/test.en  \n",
            "  inflating: spbert-main/VQUANDA/cover_processed/test.sparql  \n",
            "  inflating: spbert-main/VQUANDA/cover_processed/train.en  \n",
            "  inflating: spbert-main/VQUANDA/cover_processed/train.sparql  \n",
            "   creating: spbert-main/VQUANDA/processed/\n",
            "  inflating: spbert-main/VQUANDA/processed/dev.en  \n",
            "  inflating: spbert-main/VQUANDA/processed/dev.sparql  \n",
            "  inflating: spbert-main/VQUANDA/processed/test.en  \n",
            "  inflating: spbert-main/VQUANDA/processed/test.sparql  \n",
            "  inflating: spbert-main/VQUANDA/processed/train.en  \n",
            "  inflating: spbert-main/VQUANDA/processed/train.sparql  \n",
            "  inflating: spbert-main/VQUANDA/test.en  \n",
            "  inflating: spbert-main/VQUANDA/test.sparql  \n",
            "  inflating: spbert-main/VQUANDA/test_lower.en  \n",
            "  inflating: spbert-main/VQUANDA/test_lower.sparql  \n",
            "  inflating: spbert-main/VQUANDA/train.en  \n",
            "  inflating: spbert-main/VQUANDA/train.sparql  \n",
            "  inflating: spbert-main/VQUANDA/train_lower.en  \n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!unzip SPBERT2.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python spbert-main/run.py --do_train --do_eval --model_type bert --model_architecture bert2bert --encoder_model_name_or_path bert-base-cased --decoder_model_name_or_path razent/spbert-mlm-zero --source sparql --target en --train_filename spbert-main/LCQUAD/train --dev_filename spbert-main/LCQUAD/dev --output_dir spbert-main --max_source_length 64 --weight_decay 0.01 --max_target_length 128 --beam_size 10 --train_batch_size 32 --eval_batch_size 32 --learning_rate 5e-5 --save_inverval 10 --num_train_epochs 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mexZ8oRV4yeZ",
        "outputId": "17d29bf8-7688-47c3-873d-c9db6214d985"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-04 16:04:43.273152: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-04 16:04:44.457331: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-04 16:04:44.457981: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-04 16:04:44.459380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "03/04/2023 16:04:46 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, beam_size=10, config_name='', decoder_model_name_or_path='razent/spbert-mlm-zero', dev_filename='spbert-main/LCQUAD/dev', do_eval=True, do_lower_case=False, do_test=False, do_train=True, encoder_model_name_or_path='bert-base-cased', eval_batch_size=32, eval_steps=-1, gradient_accumulation_steps=1, learning_rate=5e-05, load_model_path=None, local_rank=-1, max_grad_norm=1.0, max_source_length=64, max_steps=-1, max_target_length=128, model_architecture='bert2bert', model_type='bert', no_cuda=False, num_train_epochs=30, output_dir='spbert-main', save_inverval=10, seed=42, source='sparql', target='en', test_filename=None, tokenizer_name='', train_batch_size=32, train_filename='spbert-main/LCQUAD/train', train_steps=-1, warmup_steps=0, weight_decay=0.01)\n",
            "03/04/2023 16:04:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False\n",
            "Downloading (…)lve/main/config.json: 100% 570/570 [00:00<00:00, 91.0kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 213k/213k [00:00<00:00, 318kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 11.4kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 436M/436M [00:06<00:00, 65.8MB/s]\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Downloading (…)lve/main/config.json: 100% 503/503 [00:00<00:00, 200kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 436M/436M [00:03<00:00, 141MB/s]\n",
            "Some weights of the model checkpoint at razent/spbert-mlm-zero were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at razent/spbert-mlm-zero and are newly initialized: ['bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   *** Example ***\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   idx: 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_tokens: ['[CLS]', 'which', 'current', 'mi', '##chi', '##gan', 'w', '##ol', '##ver', '##ines', 'team', 'member', 'debuted', 'in', 'ch', '##ica', '##go', 'bears', '?', '[SEP]']\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_ids: 101 1134 1954 1940 4313 3820 192 4063 4121 8515 1264 1420 5362 1107 22572 4578 2758 8807 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_tokens: ['[CLS]', 'select', 'distinct', 'var', '_', 'u', '##ri', 'where', 'bra', '##ck', '_', 'open', 'var', '_', 'u', '##ri', '<', 'd', '##b', '##p', '_', 'debut', '##te', '##am', '>', '<', 'd', '##b', '##r', '_', 'Chicago', '_', 'Bears', '>', 'se', '##p', '_', 'dot', 'var', '_', 'u', '##ri', '<', 'd', '##b', '##p', '_', 'current', '##te', '##am', '>', '<', 'd', '##b', '##r', '_', 'Michigan', '_', 'Wolverine', '##s', '>', 'se', '##p', '_', 'dot', 'bra', '##ck', '_', 'close', '[SEP]']\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_ids: 101 8247 4966 15661 168 190 2047 1187 12418 2158 168 1501 15661 168 190 2047 133 173 1830 1643 168 1963 1566 2312 135 133 173 1830 1197 168 2290 168 10169 135 14516 1643 168 15645 15661 168 190 2047 133 173 1830 1643 168 1954 1566 2312 135 133 173 1830 1197 168 3312 168 25862 1116 135 14516 1643 168 15645 12418 2158 168 1601 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   *** Example ***\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   idx: 1\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_tokens: ['[CLS]', 'give', 'me', 'a', 'list', 'of', 'everyone', 'who', 'married', 'the', 'musicians', 'signed', 'up', 'with', 'p', '##ab', '##lo', 'records', '?', '[SEP]']\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_ids: 101 1660 1143 170 2190 1104 2490 1150 1597 1103 4992 1878 1146 1114 185 6639 2858 3002 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_tokens: ['[CLS]', 'select', 'distinct', 'var', '_', 'u', '##ri', 'where', 'bra', '##ck', '_', 'open', 'var', '_', 'x', '<', 'd', '##b', '##p', '_', 'label', '>', '<', 'd', '##b', '##r', '_', 'Pablo', '_', 'Records', '>', 'se', '##p', '_', 'dot', 'var', '_', 'u', '##ri', '<', 'd', '##bo', '_', 'spouse', '>', 'var', '_', 'x', 'se', '##p', '_', 'dot', 'bra', '##ck', '_', 'close', '[SEP]']\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_ids: 101 8247 4966 15661 168 190 2047 1187 12418 2158 168 1501 15661 168 193 133 173 1830 1643 168 3107 135 133 173 1830 1197 168 11660 168 2151 135 14516 1643 168 15645 15661 168 190 2047 133 173 4043 168 20846 135 15661 168 193 14516 1643 168 15645 12418 2158 168 1601 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   *** Example ***\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   idx: 2\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_tokens: ['[CLS]', 'who', 'was', 'in', 'missions', 'of', 'g', '##em', '##ini', '8', 'and', 'a', '##pol', '##lo', '11', '?', '[SEP]']\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_ids: 101 1150 1108 1107 6178 1104 176 5521 4729 129 1105 170 23043 2858 1429 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_tokens: ['[CLS]', 'select', 'distinct', 'var', '_', 'u', '##ri', 'where', 'bra', '##ck', '_', 'open', 'var', '_', 'u', '##ri', '<', 'd', '##b', '##p', '_', 'mission', '>', '<', 'd', '##b', '##r', '_', 'Gemini', '_', '8', '>', 'se', '##p', '_', 'dot', 'var', '_', 'u', '##ri', '<', 'd', '##b', '##p', '_', 'mission', '>', '<', 'd', '##b', '##r', '_', 'Apollo', '_', '11', '>', 'se', '##p', '_', 'dot', 'bra', '##ck', '_', 'close', '[SEP]']\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_ids: 101 8247 4966 15661 168 190 2047 1187 12418 2158 168 1501 15661 168 190 2047 133 173 1830 1643 168 2862 135 133 173 1830 1197 168 23716 168 129 135 14516 1643 168 15645 15661 168 190 2047 133 173 1830 1643 168 2862 135 133 173 1830 1197 168 9053 168 1429 135 14516 1643 168 15645 12418 2158 168 1601 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   *** Example ***\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   idx: 3\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_tokens: ['[CLS]', 'what', 'is', 'the', 'location', 'of', 'the', 'do', '##fin', '##iv', '##ka', 'estuary', 'which', 'is', 'also', 'the', 'birthplace', 'of', 'the', 'l', '##ili', '##ya', 'lo', '##ban', '##ova', '?', '[SEP]']\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_ids: 101 1184 1110 1103 2450 1104 1103 1202 16598 11083 1968 24703 1134 1110 1145 1103 15979 1104 1103 181 18575 2315 25338 7167 8625 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_tokens: ['[CLS]', 'select', 'distinct', 'var', '_', 'u', '##ri', 'where', 'bra', '##ck', '_', 'open', '<', 'd', '##b', '##r', '_', 'Do', '##fin', '##iv', '##ka', '_', 'E', '##st', '##uary', '>', '<', 'd', '##b', '##p', '_', 'location', '>', 'var', '_', 'u', '##ri', 'se', '##p', '_', 'dot', '<', 'd', '##b', '##r', '_', 'Lil', '##iya', '_', 'Lo', '##ban', '##ova', '>', '<', 'd', '##b', '##p', '_', 'birthplace', '>', 'var', '_', 'u', '##ri', 'bra', '##ck', '_', 'close', '[SEP]']\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_ids: 101 8247 4966 15661 168 190 2047 1187 12418 2158 168 1501 133 173 1830 1197 168 2091 16598 11083 1968 168 142 2050 18487 135 133 173 1830 1643 168 2450 135 15661 168 190 2047 14516 1643 168 15645 133 173 1830 1197 168 14138 9384 168 10605 7167 8625 135 133 173 1830 1643 168 15979 135 15661 168 190 2047 12418 2158 168 1601 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   *** Example ***\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   idx: 4\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_tokens: ['[CLS]', 'which', 'division', 'of', 'the', 'run', '##ew', '##aker', 'entertainment', 'is', 'also', 'the', 'destinations', 'of', 'the', 'air', '##tour', '##s', 'international', 'air', '##ways', '?', '[SEP]']\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_ids: 101 1134 2417 1104 1103 1576 5773 17051 5936 1110 1145 1103 15309 1104 1103 1586 18834 1116 1835 1586 8520 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   source_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_tokens: ['[CLS]', 'select', 'distinct', 'var', '_', 'u', '##ri', 'where', 'bra', '##ck', '_', 'open', '<', 'd', '##b', '##r', '_', 'Run', '##ew', '##aker', '_', 'Entertainment', '>', '<', 'd', '##bo', '_', 'division', '>', 'var', '_', 'u', '##ri', 'se', '##p', '_', 'dot', '<', 'd', '##b', '##r', '_', 'Air', '##tour', '##s', '_', 'International', '_', 'Airways', '>', '<', 'd', '##b', '##p', '_', 'destinations', '>', 'var', '_', 'u', '##ri', 'bra', '##ck', '_', 'close', '[SEP]']\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_ids: 101 8247 4966 15661 168 190 2047 1187 12418 2158 168 1501 133 173 1830 1197 168 6728 5773 17051 168 4549 135 133 173 4043 168 2417 135 15661 168 190 2047 14516 1643 168 15645 133 173 1830 1197 168 1806 18834 1116 168 1570 168 14099 135 133 173 1830 1643 168 15309 135 15661 168 190 2047 12418 2158 168 1601 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/04/2023 16:05:19 - INFO - __main__ -   target_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "03/04/2023 16:05:22 - INFO - __main__ -   ***** Running training *****\n",
            "03/04/2023 16:05:22 - INFO - __main__ -     Num examples = 4000\n",
            "03/04/2023 16:05:22 - INFO - __main__ -     Batch size = 32\n",
            "03/04/2023 16:05:22 - INFO - __main__ -     Num epoch = 30\n",
            "epoch 0 loss 6.183: 100% 125/125 [02:43<00:00,  1.31s/it]\n",
            "finished\n",
            "epoch 1 loss 4.4051: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 2 loss 3.7096: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 3 loss 3.3265: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 4 loss 3.0763: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 5 loss 2.8953: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 6 loss 2.7562: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 7 loss 2.6441: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 8 loss 2.5512: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 9 loss 2.4722: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "starting evaluation\n",
            "we are in the else condition\n",
            "before eval_sampler\n",
            "before batch_eval_dataloader\n",
            "before train\n",
            "03/04/2023 16:51:28 - INFO - __main__ -     BLEU = 22.5209 \n",
            "03/04/2023 16:51:28 - INFO - __main__ -     ********************\n",
            "03/04/2023 16:51:28 - INFO - __main__ -     Best bleu:22.520894574841932\n",
            "03/04/2023 16:51:28 - INFO - __main__ -     ********************\n",
            "ECCOCI\n",
            "spbert-main/checkpoint-best-bleu/pytorch_model.bin\n",
            "epoch 10 loss 1.7202: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 11 loss 1.7019: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 12 loss 1.6852: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 13 loss 1.6699: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 14 loss 1.6558: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 15 loss 1.6426: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 16 loss 1.6305: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 17 loss 1.6192: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 18 loss 1.6088: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 19 loss 1.5991: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "starting evaluation\n",
            "before eval_sampler\n",
            "before batch_eval_dataloader\n",
            "before train\n",
            "03/04/2023 17:37:12 - INFO - __main__ -     BLEU = 25.4299 \n",
            "03/04/2023 17:37:12 - INFO - __main__ -     ********************\n",
            "03/04/2023 17:37:12 - INFO - __main__ -     Best bleu:25.429929417955883\n",
            "03/04/2023 17:37:12 - INFO - __main__ -     ********************\n",
            "ECCOCI\n",
            "spbert-main/checkpoint-best-bleu/pytorch_model.bin\n",
            "epoch 20 loss 1.4992: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 21 loss 1.4942: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 22 loss 1.4897: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 23 loss 1.4855: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 24 loss 1.4817: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 25 loss 1.4784: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 26 loss 1.4753: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 27 loss 1.4726: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 28 loss 1.4702: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "epoch 29 loss 1.4682: 100% 125/125 [02:41<00:00,  1.29s/it]\n",
            "finished\n",
            "starting evaluation\n",
            "before eval_sampler\n",
            "before batch_eval_dataloader\n",
            "before train\n",
            "03/04/2023 18:22:57 - INFO - __main__ -     BLEU = 29.7352 \n",
            "03/04/2023 18:22:57 - INFO - __main__ -     ********************\n",
            "03/04/2023 18:22:57 - INFO - __main__ -     Best bleu:29.735225570946298\n",
            "03/04/2023 18:22:57 - INFO - __main__ -     ********************\n",
            "ECCOCI\n",
            "spbert-main/checkpoint-best-bleu/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ww8bdp0K3Yi3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}